# Gu√≠a: Escalar Nodos a 0 para Ahorrar Dinero (Proyecto de Portafolio)

> **Contexto**: Esta gu√≠a est√° dise√±ada espec√≠ficamente para proyectos de portafolio donde el ahorro de costos es importante y un delay de 2-3 minutos al reactivar es aceptable.

**Problema**: Aunque los pods est√©n en 0 r√©plicas, los nodos EC2 siguen activos y AWS te cobra por ellos.

**Soluci√≥n**: Configurar Cluster Autoscaler para que elimine los nodos cuando todos los pods est√©n en 0, optimizando costos para un proyecto de portafolio.

---

## üìã Checklist R√°pido

- [ ] Cluster Autoscaler instalado y funcionando
- [ ] Nodegroup configurado con `nodes-min: 0`
- [ ] Cluster Autoscaler configurado para permitir escalado a 0
- [ ] KEDA Cron configurado para escalar pods a 0 (L-V 19:00-09:00 + fines de semana)
- [ ] Verificado que el escalado funciona correctamente

---

## üéØ Objetivo de esta Configuraci√≥n

**Horario Laboral (Lunes-Viernes 09:00-19:00)**:
- ‚úÖ Pods activos (1 r√©plica m√≠nima)
- ‚úÖ Nodos activos (1 nodo m√≠nimo)

**Horario No Laboral (Lunes-Viernes 19:00-09:00 + Fines de Semana)**:
- ‚ùå Pods a 0 r√©plicas (KEDA Cron)
- ‚ùå Nodos a 0 (Cluster Autoscaler)
- üí∞ **Ahorro**: $0 en EC2 durante horarios inactivos

---

## üìù Gu√≠a Paso a Paso

### Paso 1: Verificar que KEDA Cron est√° Configurado

**Antes de configurar Cluster Autoscaler, aseg√∫rate de que KEDA Cron ya est√° configurado para escalar pods a 0.**

```powershell
# Verificar que todos los ScaledObjects est√°n aplicados
kubectl get scaledobjects -n production

# Verificar horario configurado en un ScaledObject
kubectl describe scaledobject jobber-gateway-keda-scaledobject -n production
```

**Debes ver:**
- `start: "0 9 * * 1-5"` (09:00 Lunes-Viernes)
- `end: "0 19 * * 1-5"` (19:00 Lunes-Viernes)
- `minReplicaCount: 0`

**Si KEDA Cron no est√° configurado**, sigue primero la gu√≠a: `../README-CRON.md`

---

### Paso 2: Verificar o Instalar Cluster Autoscaler

#### 2.1. Verificar si Cluster Autoscaler est√° instalado

```powershell
# Verificar que Cluster Autoscaler est√° corriendo
kubectl -n kube-system get pods -l app=cluster-autoscaler

# Ver logs para confirmar que est√° funcionando
kubectl -n kube-system logs -f deployment/cluster-autoscaler --tail=50
```

**Resultado esperado:**
```
NAME                                  READY   STATUS    RESTARTS   AGE
cluster-autoscaler-xxxxxxxxxx-xxxxx   1/1     Running   0          5d
```

#### 2.2. Si no est√° instalado, instalarlo

**Si Cluster Autoscaler no est√° instalado**, sigue la gu√≠a completa: `../../README-CLUSTER-AUTOSCALER.md`

**Resumen r√°pido:**
1. Crear pol√≠tica IAM para Cluster Autoscaler
2. Asociar OIDC provider
3. Crear Service Account con permisos
4. Instalar Cluster Autoscaler
5. Configurar tags en Auto Scaling Groups

---

### Paso 3: Configurar Nodegroup para Permitir 0 Nodos

#### 3.1. Ver nodegroup actual

```powershell
# Ver informaci√≥n del nodegroup
eksctl get nodegroup --cluster jobberapp-demo --region us-east-1
```

**Anota el nombre de tu nodegroup** (ej: `demo-small`, `default`, etc.)

#### 3.2. Actualizar nodegroup (permitir 0 nodos)

```powershell
# ‚ö†Ô∏è IMPORTANTE: Reemplaza "demo-small" con el nombre de TU nodegroup
# Reemplaza "jobberapp-demo" con el nombre de TU cl√∫ster
# Reemplaza "us-east-1" con tu regi√≥n

eksctl scale nodegroup \
  --cluster jobberapp-demo \
  --name demo-small \
  --nodes-min 0 \
  --nodes-max 3 \
  --region us-east-1
```

**Resultado esperado:**
```
[‚Ñπ]  scaling nodegroup "demo-small" in cluster "jobberapp-demo"
[‚úì]  scaling nodegroup "demo-small" in cluster "jobberapp-demo" succeeded
```

#### 3.3. Verificar que se actualiz√≥ correctamente

```powershell
eksctl get nodegroup --cluster jobberapp-demo --region us-east-1
```

**Debes ver `MIN: 0` en la salida:**
```
CLUSTER          NODEGROUP    STATUS  MIN  MAX  DESIRED  INSTANCE TYPE
jobberapp-demo   demo-small   ACTIVE  0    3    1        t3.medium
```

---

### Paso 4: Configurar Cluster Autoscaler para Escalar a 0

#### 4.1. Editar el Deployment del Cluster Autoscaler

```powershell
kubectl -n kube-system edit deployment cluster-autoscaler
```

Esto abrir√° el editor (vim por defecto).

#### 4.2. Buscar la secci√≥n `args:`

Usa `/args` para buscar la secci√≥n de argumentos.

#### 4.3. Agregar/Modificar estos argumentos

**Busca la secci√≥n:**
```yaml
spec:
  template:
    spec:
      containers:
      - name: cluster-autoscaler
        args:
```

**Agrega o modifica los argumentos para que queden as√≠:**
```yaml
spec:
  template:
    spec:
      containers:
      - name: cluster-autoscaler
        args:
        - --balance-similar-node-groups
        - --skip-nodes-with-system-pods=false      # ‚ö†Ô∏è CR√çTICO: Permite eliminar nodos con pods del sistema
        - --skip-nodes-with-local-storage=false    # Permite eliminar nodos con storage local
        - --scale-down-delay-after-add=10m         # Esperar 10 min despu√©s de agregar nodo
        - --scale-down-unneeded-time=10m           # Esperar 10 min antes de eliminar nodo no usado
        - --scale-down-utilization-threshold=0.5    # Eliminar nodo si uso < 50%
        - --max-node-provision-time=15m            # Tiempo m√°ximo para crear nodo
```

**‚ö†Ô∏è IMPORTANTE**: 
- `--skip-nodes-with-system-pods=false` es **CR√çTICO** - permite eliminar nodos incluso si tienen pods del sistema (como `kube-proxy`, `aws-node`)
- Esta configuraci√≥n es adecuada para proyectos de portafolio donde el ahorro de costos es prioritario
- En producci√≥n cr√≠tica, considera mantener 1 nodo m√≠nimo

#### 4.4. Guardar y salir

**En vim:**
1. Presiona `Esc` para salir del modo inserci√≥n
2. Escribe `:wq` y presiona `Enter` para guardar y salir

**En nano:**
1. Presiona `Ctrl + O` para guardar
2. Presiona `Ctrl + X` para salir

#### 4.5. Verificar que el Deployment se actualiz√≥

```powershell
# Ver el Deployment actualizado
kubectl -n kube-system get deployment cluster-autoscaler -o yaml | Select-String -Pattern "skip-nodes-with-system-pods"

# Verificar que el pod se reinici√≥ con la nueva configuraci√≥n
kubectl -n kube-system get pods -l app=cluster-autoscaler
```

**Debes ver el pod reinici√°ndose o ya reiniciado con la nueva configuraci√≥n.**

---

### Paso 5: Verificar que Todo Est√° Configurado Correctamente

#### 5.1. Verificar configuraci√≥n completa

```powershell
# 1. Verificar nodegroup (debe tener MIN: 0)
eksctl get nodegroup --cluster jobberapp-demo --region us-east-1

# 2. Verificar Cluster Autoscaler est√° corriendo
kubectl -n kube-system get pods -l app=cluster-autoscaler

# 3. Verificar argumentos del Cluster Autoscaler
kubectl -n kube-system get deployment cluster-autoscaler -o jsonpath='{.spec.template.spec.containers[0].args}' | ConvertFrom-Json

# 4. Verificar KEDA Cron est√° configurado
kubectl get scaledobjects -n production
```

#### 5.2. Verificar logs del Cluster Autoscaler

```powershell
kubectl -n kube-system logs -f deployment/cluster-autoscaler --tail=100
```

**Busca mensajes como:**
- `Successfully registered cluster-autoscaler with cloud provider`
- `Node group demo-small: minSize=0, maxSize=3, currentSize=1`
- `skip-nodes-with-system-pods=false` (debe aparecer en los logs)

---

### Paso 6: Probar el Escalado Autom√°tico

#### 6.1. Monitorear en tiempo real (recomendado)

**Abre 3 terminales diferentes:**

**Terminal 1: Monitorear pods**
```powershell
kubectl get pods -n production -w
```

**Terminal 2: Monitorear nodos**
```powershell
kubectl get nodes -w
```

**Terminal 3: Logs del Cluster Autoscaler**
```powershell
kubectl -n kube-system logs -f deployment/cluster-autoscaler
```

#### 6.2. Esperar al horario de apagado (19:00) o probar manualmente

**Opci√≥n A: Esperar al horario real (19:00)**
- A las 19:00, KEDA Cron escalar√° los pods a 0
- Despu√©s de 10 minutos (19:10), Cluster Autoscaler escalar√° los nodos a 0

**Opci√≥n B: Probar manualmente (para verificar que funciona)**

```powershell
# 1. Escalar manualmente un deployment a 0 para simular
kubectl scale deployment jobber-gateway -n production --replicas=0

# 2. Esperar 10 minutos y verificar que el nodo se escala a 0
kubectl get nodes
eksctl get nodegroup --cluster jobberapp-demo --region us-east-1

# 3. Escalar de vuelta a 1 para reactivar
kubectl scale deployment jobber-gateway -n production --replicas=1

# 4. Esperar 2-3 minutos y verificar que el nodo se crea
kubectl get nodes
```

---

## üîÑ C√≥mo Funciona el Escalado Autom√°tico Completo

### Flujo Completo: Horario No Laboral (L-V 19:00-09:00 + Fines de Semana)

**Lunes-Viernes a las 19:00:**
1. ‚è∞ **19:00** ‚Üí KEDA Cron detecta que es hora de apagar
2. üìâ **19:00** ‚Üí KEDA Cron escala **todos los pods a 0 r√©plicas**
3. üñ•Ô∏è **19:00-19:10** ‚Üí Nodos quedan vac√≠os (solo pods del sistema como `kube-system`)
4. üëÄ **19:10** ‚Üí Cluster Autoscaler detecta nodos subutilizados
5. ‚è≥ **19:10** ‚Üí CA espera 10 minutos (configurado en `--scale-down-unneeded-time`)
6. üóëÔ∏è **19:20** ‚Üí CA escala el **nodegroup a 0** ‚Üí Nodos EC2 se eliminan
7. üí∞ **Ahorro**: No pagas por instancias EC2 durante la noche

**Fines de Semana (S√°bado y Domingo):**
- Los pods ya est√°n en 0 (KEDA Cron los mantiene en 0)
- Los nodos tambi√©n est√°n en 0 (Cluster Autoscaler los mantiene en 0)
- üí∞ **Ahorro total**: $0 en EC2 durante todo el fin de semana

### Flujo Completo: Horario Laboral (L-V 09:00-19:00)

**Lunes-Viernes a las 09:00:**
1. ‚è∞ **09:00** ‚Üí KEDA Cron detecta que es hora de activar
2. üìà **09:00** ‚Üí KEDA Cron escala **pods a 1 r√©plica** (m√≠nimo)
3. ‚è≥ **09:00** ‚Üí Pods quedan en estado `Pending` (no hay nodos todav√≠a)
4. üëÄ **09:00** ‚Üí Cluster Autoscaler detecta pods pendientes
5. üÜï **09:00-09:03** ‚Üí CA escala el **nodegroup a 1** ‚Üí AWS crea nuevo nodo EC2 (2-3 minutos)
6. ‚úÖ **09:03** ‚Üí Pods se programan en el nuevo nodo
7. üöÄ **09:03** ‚Üí Aplicaci√≥n est√° disponible y funcionando

**Durante el d√≠a (09:00-19:00):**
- Pods activos (1 r√©plica m√≠nima, puede escalar m√°s si hay carga)
- Nodos activos (1 nodo m√≠nimo, puede escalar m√°s si hay carga)
- Aplicaci√≥n disponible para uso

---

## ‚úÖ Verificaci√≥n del Funcionamiento

---

### 7.1. Verificar estado del nodegroup

### 5.1. Monitorear en tiempo real

**Terminal 1: Monitorear pods**
```powershell
kubectl get pods -n production -w
```

**Terminal 2: Monitorear nodos**
```powershell
kubectl get nodes -w
```

**Terminal 3: Logs del Cluster Autoscaler**
```powershell
kubectl -n kube-system logs -f deployment/cluster-autoscaler
```

### 5.2. Verificar estado del nodegroup

```powershell
eksctl get nodegroup --cluster jobberapp-demo --region us-east-1
```

**Salida esperada cuando est√° en 0:**
```
CLUSTER          NODEGROUP    STATUS  MIN  MAX  DESIRED  INSTANCE TYPE  IMAGE ID
jobberapp-demo   demo-small   ACTIVE  0    3    0        t3.medium      AL2_x86_64
```

### 5.3. Logs esperados


**Cuando escala de 0 a 1 (09:00):**
```
I0920 09:00:00.123456       1 cluster_autoscaler.go:1234] Scale up: 1 node(s) needed
I0920 09:00:05.654321       1 cluster_autoscaler.go:1235] Node i-0987654321fedcba0 created
```

---

## 8. ‚ö†Ô∏è Consideraciones Importantes

### 6.1. Limitaciones

1. **Pods del sistema**: Si tienes pods cr√≠ticos en `kube-system` (como `cluster-autoscaler` mismo), el CA puede no escalar a 0 completamente
2. **DaemonSets**: Los DaemonSets (como `kube-proxy`, `aws-node`) siempre corren en cada nodo
3. **Tiempo de arranque**: Cuando se reactiva, toma **2-3 minutos** crear el nodo y programar los pods

### 6.2. Soluciones

1. **Usar `--skip-nodes-with-system-pods=false`** (ya configurado arriba)
2. **Mover pods cr√≠ticos a nodos dedicados** (no escalables)
3. **Aceptar que siempre habr√° 1 nodo m√≠nimo** si tienes DaemonSets cr√≠ticos

---

## 9. üí∞ Ahorro Estimado

### C√°lculo detallado

**Costo t3.medium**: ~$0.0416/hora = **~$30/mes** (720 horas)

**Horario activo (L-V 09:00-19:00)**:
- D√≠as laborables: ~22 d√≠as/mes
- Horas activas: 10 horas/d√≠a √ó 22 d√≠as = **220 horas/mes**
- Costo activo: 220 horas √ó $0.0416 = **$9.15/mes**

**Horario inactivo (19:00-09:00 + fines de semana)**:
- Horas inactivas: 720 - 220 = **500 horas/mes**
- Costo si se apaga: **$0/mes**
- **Ahorro potencial**: 500 horas √ó $0.0416 = **$20.80/mes**

### Escenario 1: Nodos a 0 (recomendado para proyecto de portafolio) ‚≠ê

- **Horario activo**: $9.15/mes
- **Horario inactivo**: $0/mes
- **Total**: **~$9.15/mes**
- **Ahorro vs siempre encendido**: **$20.85/mes (69% de ahorro)**
- **Ahorro anual**: **~$250/a√±o**

**Ventajas para proyecto de portafolio**:
- ‚úÖ Ahorro significativo ($20.85/mes = $250/a√±o) - importante para proyectos personales
- ‚úÖ Demuestra conocimiento avanzado de autoscaling en entrevistas t√©cnicas
- ‚úÖ Muestra habilidades de cost optimization en AWS
- ‚úÖ El delay de 2-3 minutos no es cr√≠tico para un proyecto de portafolio
- ‚úÖ Demuestra integraci√≥n de m√∫ltiples tecnolog√≠as (KEDA + Cluster Autoscaler)

**Desventajas**:
- ‚ö†Ô∏è Delay de 2-3 minutos al reactivar (aceptable para proyecto de portafolio)
- ‚ö†Ô∏è Requiere configuraci√≥n adicional del Cluster Autoscaler

### Escenario 2: Mantener 1 nodo m√≠nimo (m√°s simple)

- **Siempre**: 1 nodo t3.medium = **~$30/mes**
- **Ventaja**: No hay delay de arranque, m√°s seguro, configuraci√≥n m√°s simple

**Cu√°ndo usar**:
- Si necesitas disponibilidad inmediata 24/7
- Si el delay de 2-3 minutos es inaceptable
- Si prefieres simplicidad sobre ahorro

### üéØ Recomendaci√≥n para Proyecto de Portafolio

**‚úÖ S√ç vale la pena escalar nodos a 0 en un proyecto de portafolio** por estas razones:

1. **Ahorro significativo**: $20.85/mes ($250/a√±o) es considerable para un proyecto personal de portafolio
2. **Demuestra conocimiento t√©cnico avanzado en entrevistas**: 
   - Integraci√≥n KEDA + Cluster Autoscaler
   - Cost optimization en AWS
   - Autoscaling completo (pods + nodos)
   - Conocimiento de mejores pr√°cticas de Kubernetes
3. **Valor para el portafolio**: Es una caracter√≠stica t√©cnica impresionante que diferencia tu proyecto de otros portafolios b√°sicos
4. **El delay es aceptable**: 2-3 minutos de espera al reactivar es perfectamente aceptable para un proyecto de portafolio (no es un servicio cr√≠tico 24/7)
5. **Alineado con uso real**: Si tu proyecto de portafolio solo se usa en horario laboral, no tiene sentido pagar por las noches y fines de semana

**Conclusi√≥n**: Para un proyecto de portafolio, **escalar nodos a 0 es la mejor opci√≥n** porque:
- Demuestra habilidades avanzadas de DevOps y cost optimization
- Ahorra dinero significativo sin sacrificar funcionalidad cr√≠tica
- Es un excelente punto de conversaci√≥n en entrevistas t√©cnicas
- Muestra que entiendes c√≥mo optimizar costos en la nube

---

## 10. Troubleshooting

| Problema | Soluci√≥n |
|----------|----------|
| Nodos no escalan a 0 | Verificar que `nodes-min: 0` est√° configurado |
| CA no elimina nodos | Verificar logs del CA, puede estar esperando el delay (10 min) |
| Pods quedan Pending al reactivar | Normal, esperar 2-3 minutos para que el nodo se cree |
| CA no crea nodos | Verificar permisos IAM y tags del ASG |

---

## 11. Comandos √ötiles

```powershell
# Ver estado actual
kubectl get nodes
kubectl get pods -n production
eksctl get nodegroup --cluster jobberapp-demo --region us-east-1

# Forzar escalado manual (si es necesario)
eksctl scale nodegroup --cluster jobberapp-demo --name demo-small --nodes 0 --region us-east-1

# Ver logs del CA
kubectl -n kube-system logs -f deployment/cluster-autoscaler

# Ver eventos recientes
kubectl get events -n production --sort-by='.lastTimestamp' | Select-Object -Last 20
```

---

## 12. Referencias

- [Cluster Autoscaler AWS](https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler/cloudprovider/aws)
- [EKS Best Practices - Autoscaling](https://aws.github.io/aws-eks-best-practices/cluster-autoscaling/)
- [Cluster Autoscaler - Scale Down FAQ](https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/FAQ.md#how-does-scale-down-work)

---

**‚úÖ Listo**: Con esta configuraci√≥n, cuando KEDA Cron escala los pods a 0, el Cluster Autoscaler autom√°ticamente eliminar√° los nodos despu√©s de 10 minutos, ahorrando dinero en EC2. Esta configuraci√≥n es ideal para proyectos de portafolio donde el ahorro de costos y la demostraci√≥n de habilidades t√©cnicas avanzadas son prioridades.

